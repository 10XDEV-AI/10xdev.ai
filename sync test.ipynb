{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a4f9fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syncing AI :\n",
      "Files and directories to ignore:\n",
      "['apiKeys.js']\n",
      "Directories: ['images']\n",
      "Directories: []\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'file_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file_path'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 134\u001b[0m\n\u001b[1;32m    130\u001b[0m     create_clone(read_info())\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m \u001b[43msyncAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 81\u001b[0m, in \u001b[0;36msyncAI\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m                 file_paths_details\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, filename))\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Find the set difference between file_paths_details and df4[\"filepath\"]\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m new_file_paths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(file_paths_details) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mfs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile_path\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# Iterate over the new_file_paths set and create a new row for each file path\u001b[39;00m\n\u001b[1;32m     83\u001b[0m new_rows \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'file_path'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai\n",
    "import time\n",
    "import numpy\n",
    "from utilities.readInfo import read_info\n",
    "from utilities.embedding import get_embedding\n",
    "from utilities.create_clone import create_clone, get_clone_path\n",
    "import chardet\n",
    "\n",
    "text_file = open(\"API_key.txt\", \"r\")\n",
    "openai.api_key =  text_file.read()\n",
    "text_file.close()\n",
    "\n",
    "fs = pd.DataFrame()\n",
    "\n",
    "def get_diff(old_file_path, new_file_path):\n",
    "    result = subprocess.run([\"diff\", old_file_path, new_file_path], capture_output=True, text=True)\n",
    "    return result.stdout\n",
    "\n",
    "def summarize_str(filename,string):\n",
    "    while True:\n",
    "        try:\n",
    "             response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"system\", \"content\": \"Summarize what this file in the codebase does, assume context when neccessary.\"},\n",
    "                      {\"role\": \"user\", \"content\": \"File \"+filename+\" has \"+string}],\n",
    "                temperature=0,\n",
    "                max_tokens=256\n",
    "             )\n",
    "             return response[\"choices\"][0][\"message\"]['content']\n",
    "        except Exception as e:\n",
    "             print(f\"Encountered error: {e}\")\n",
    "             print(\"Retrying in 20 seconds...\")\n",
    "             time.sleep(20)\n",
    "\n",
    "def resync(filename):\n",
    "    root = read_info()\n",
    "    with open(os.path.join(root, filename), 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    if not(result['encoding'] == 'ascii' or result['encoding'] == 'ISO-8859-1'):\n",
    "        print(\"File \"+file+\" was not summarised as it is not a text file\")\n",
    "        return \"Ignore\"\n",
    "    full_file_path = os.path.join(path, file)\n",
    "    with open(full_file_path, 'r') as f:\n",
    "        file_contents = f.read()\n",
    "    if num_tokens_from_string(file_contents) > 4096:\n",
    "        print(\"File too large to summarize. Splitting file into blocks.\")\n",
    "        blocks = split_file_into_blocks(file)\n",
    "        summaries = []\n",
    "        for block in blocks:\n",
    "            summaries.append(summarize_str(file,block))\n",
    "        return \"\\n\".join(summaries)\n",
    "    return summarize_str(file,file_contents)\n",
    "\n",
    "def syncAI():\n",
    "    path = read_info()\n",
    "    global fs\n",
    "\n",
    "    print(\"Syncing AI :\")\n",
    "    file_paths_details = []\n",
    "    Files_to_ignore = open(path+\"/.AIIgnore\", \"r\").read().splitlines()\n",
    "    print(\"Files and directories to ignore:\")\n",
    "    print(Files_to_ignore)\n",
    "\n",
    "    for root, directories, files in os.walk(path):\n",
    "        # Exclude any directories that appear in the ignore list\n",
    "        directories[:] = [d for d in directories if d not in Files_to_ignore]\n",
    "        print(\"Directories:\", directories)\n",
    "        for filename in files:\n",
    "            if filename not in Files_to_ignore:\n",
    "                #print(\"Analyzing : \"+filename)\n",
    "                with open(os.path.join(root, filename), 'rb') as f:\n",
    "                    result = chardet.detect(f.read())\n",
    "                    #print(result['encoding'])\n",
    "                if result['encoding'] == 'ascii' or result['encoding'] == 'ISO-8859-1':\n",
    "                    file_paths_details.append(os.path.join(root, filename))\n",
    "\n",
    "    # Find the set difference between file_paths_details and df4[\"filepath\"]\n",
    "    new_file_paths = set(file_paths_details) - set(fs[\"file_path\"])\n",
    "    # Iterate over the new_file_paths set and create a new row for each file path\n",
    "    new_rows = []\n",
    "    for file_path in new_file_paths:\n",
    "        # Create a dictionary with the values for each column in the new row\n",
    "        row_dict = {\n",
    "            \"filepath\": file_path\n",
    "            # Add any other columns you need for the new row\n",
    "        }\n",
    "        # Append the new row to the new_rows list\n",
    "        new_rows.append(row_dict)\n",
    "        print(\"New File : \"+file_path)\n",
    "\n",
    "    # Convert the new_rows list of dictionaries to a pandas DataFrame\n",
    "    new_fs = pd.DataFrame(new_rows)\n",
    "    fs = pd.concat([fs, new_fs], ignore_index=True)\n",
    "    del_file_paths = set(fs[\"file_path\"]) - set(file_paths_details)\n",
    "\n",
    "    # Iterate over the del_file_paths set and remove the corresponding rows from dataframes\n",
    "    for file_path in del_file_paths:\n",
    "        fs = fs[fs[\"file_path\"] != file_path]\n",
    "        print(\"Deleted File : \"+file_path)\n",
    "\n",
    "    for ind in fs.index:\n",
    "        print(fs['filepath'][ind])\n",
    "        resync(fs['filepath'][ind])\n",
    "\n",
    "    if(len(new_fs)>0):\n",
    "        line_embeddings = []\n",
    "        for i in range(0,len(new_fs)):\n",
    "            filename = new_fs.iloc[i][0]\n",
    "            with open(filename, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "                line_number = 0\n",
    "                for line in lines:\n",
    "                    line_embeddings.append([filename])\n",
    "                    line_number += 1\n",
    "\n",
    "        add2fs = pd.DataFrame(line_embeddings)\n",
    "        add2fs.columns = [\"file_path\",\"\",\"\"]\n",
    "        add2df['embedding'] = ''\n",
    "\n",
    "        i=0\n",
    "        for ind in add2df.index:\n",
    "                i+=1\n",
    "                add2df['embedding'][ind] = get_embedding(add2df['Code'][ind],0.5)\n",
    "                print(round(100*i/len(add2df)))\n",
    "        print(\"Done\")\n",
    "\n",
    "    create_clone(read_info())\n",
    "\n",
    "    return\n",
    "\n",
    "syncAI()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
